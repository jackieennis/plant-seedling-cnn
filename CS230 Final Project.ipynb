{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CS230 Final Project.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"yCQtbnFZ24ye","colab_type":"text"},"cell_type":"markdown","source":["# Multiclass Plant Image Classification Using A Convolutional Neural Network\n","For our project, we aim to predict the maturity and species of plant seedling images using a convolutional neural network model. To accomplish this, we implement a convolutional neural network model in Tensorflow."]},{"metadata":{"id":"7QZe8ZyUa8QV","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","import os\n","import shutil\n","import logging\n","import random\n","\n","from PIL import Image\n","from tqdm import tqdm\n","\n","from google.colab import drive\n","\n","import tensorflow as tf\n","from tqdm import trange\n","from sklearn import preprocessing\n","\n","#drive.mount('/content/drive',force_remount=True)\n","\n","# desired image size (64) and directory paths\n","DIMENSIONS = 64\n","DRIVE_FOLDER = '/content/drive/My Drive/Colab Notebooks'\n","MAIN_FOLDER = 'all_seedlings'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kL8kjwYSYMpl","colab_type":"text"},"cell_type":"markdown","source":["## Preparing the dataset\n","### Resizing, renaming, and splitting the data\n","Our model will be trained on an image dataset provided by Kaggle to classify 12 types of plant seedlings, captured at different stages of growth. To prepare the data for input into the model, we have to perform a number of tasks:\n","* Systematically rename all the images to be able to access both the image ID and class (species) label \n","  * The seedling dataset comes in the following format:\n","    \n","    `all_seedlings`/\n","        ...\n","        {speciesLabel}_{imageID}.png\n","        May Flower_123.png\n","        ...\n","* Downsize the images to standard dimensions: 64 x 64 \n","  * Resizing the image size standardizes the images for input and makes training faster by minimizing image loading and processing time.\n","* Split the dataset into train/dev/test sets\n","  * Because we don't have a lot of images and we want the statistics on the dev set to be as representative as possible, we'll take 10% of training data as the dev_set and another 10% as the test_set.\n","* Load the data into Tensorflow vectors\n","* Encode the species labels to numerical types\n"]},{"metadata":{"id":"udGf03qwa_Rz","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","BUILD_DATASET(dims):\n"," - dims: desired image size\n","\n","Prepares a dataset of dims x dims-sized images for input into the model by:\n"," - resizing the images\n"," - renaming their file paths\n"," - moving all classes into the same folder\n","''' \n","def build_dataset(dims):\n","\n","  # get list of individual species folders\n","  species = os.listdir(os.path.join(DRIVE_FOLDER, 'seedlings'))\n","\n","  count = 0\n","  for folder in species:\n","    \n","    # get path to the folder\n","    path = os.path.join(DRIVE_FOLDER, 'seedlings', str(folder))\n","\n","    # create a list of all images in the folder\n","    images = os.listdir(path)\n","    \n","    # move the images to the same folder (\"all_seedlings\")\n","    for img_name in images:\n","\n","        # original source path to image\n","        src = os.path.join(path, img_name)\n","        \n","        # open image\n","        image = Image.open(src)\n","        \n","        # resize image using bilinear interpolation instead of nearest neighbors\n","        resized_image = image.resize((DIMENSIONS, DIMENSIONS), Image.BILINEAR)\n","  \n","        # change the file name to include class (species name) \n","        new_img_name = str(folder) + '_' + img_name\n","        \n","        # destination path to image\n","        dst = os.path.join(DRIVE_FOLDER, MAIN_FOLDER, new_img_name)\n","        \n","        resized_image.save(dst)\n","        \n","        count += 1\n","        \n","    print(\"Resized\", count, \"images.\")\n","\n","      \n","        \n","build_dataset(DIMENSIONS)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iHWMXPoSgH1X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"14af4687-d644-417e-d5c1-b8dd0808f389","executionInfo":{"status":"ok","timestamp":1550642668315,"user_tz":-60,"elapsed":504,"user":{"displayName":"Jacqueline Ennis","photoUrl":"https://lh6.googleusercontent.com/-V_6i4Q83WGo/AAAAAAAAAAI/AAAAAAAADjY/vcjM0UgblwE/s64/photo.jpg","userId":"16264072060146161880"}}},"cell_type":"code","source":["print(\"There are\", len(os.listdir(os.path.join(DRIVE_FOLDER, MAIN_FOLDER))), \"total images in the original dataset.\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["There are 5539 total images in the original dataset.\n"],"name":"stdout"}]},{"metadata":{"id":"j7vlCdUaR3V3","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","GET_SPECIES(filename):\n"," - filename: The filename of an image, in the format {speciesLabel}_{imageID}.png\n","Extracts the species class label from an image filename.\n","'''\n","def get_species(filename):\n","  species, img_id = filename.split('_')\n","  return species"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8tbn8N71eakG","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","SPLIT_TRAIN_DEV_TEST(data, train_size):\n"," - data: Original dataset to be split into training, dev, and test sets\n"," - train_size: Fraction of dataset to be devoted to training data\n"," Splits a dataset into a training, dev, and test sets given a specified training set size.\n"," The dev and test sets are equally sized from the remaining data. \n","'''\n","def split_train_dev_test(data, train_size):\n","\n","  # make sure to always shuffle with a fixed seed so that the split is reproducible\n","  random.seed(101)\n","  data.sort()\n","  random.shuffle(data)\n","\n","  # split the images in 80% train, 10% dev, 10% test\n","  pivot = int((1 - train_size) * len(data))\n","  train_filenames = data[pivot:]\n","  pivot_half = int(pivot / 2)\n","  test_filenames = data[0:pivot_half]\n","  dev_filenames = data[pivot_half:pivot]\n","  \n","  split_filenames = {'train': train_filenames,\n","              'dev': dev_filenames,\n","              'test': test_filenames}        \n","              \n","  for split in ['train', 'dev', 'test']:\n","    # create directory to hold new folder\n","    output_dir_split = os.path.join(DRIVE_FOLDER, split)\n","\n","    if not os.path.exists(output_dir_split):\n","      os.mkdir(output_dir_split)\n","    else:\n","      print(\"Warning: dir {} already exists\".format(output_dir_split))\n","\n","    print(\"Saving preprocessed data to {}\".format(output_dir_split))\n","    \n","    for img in split_filenames[split]:\n","      src = os.path.join(DRIVE_FOLDER, MAIN_FOLDER, img)\n","      dest = os.path.join(output_dir_split, img)\n","\n","      shutil.copyfile(src,dest)\n","      print('.')\n","\n","# get all images from the new standard folder\n","dataset = os.listdir(os.path.join(DRIVE_FOLDER, MAIN_FOLDER))\n","print(\"There are\", len(dataset), \"total images in this dataset.\")\n","\n","t = 0.8\n","split_train_dev_test(dataset, t)\n","\n","trainers = os.listdir(os.path.join(DRIVE_FOLDER, \"train\"))\n","print(\"There are\", len(trainers), \"training images.\")\n","\n","devs = os.listdir(os.path.join(DRIVE_FOLDER, \"dev\"))\n","print(\"There are\", len(devs), \"development images.\")\n","\n","testers = os.listdir(os.path.join(DRIVE_FOLDER, \"test\"))\n","print(\"There are\", len(testers), \"test images.\")\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cXMRvexHb9zv","colab_type":"code","outputId":"f0451936-394a-40d9-934b-c473a79fc659","executionInfo":{"status":"ok","timestamp":1550642677202,"user_tz":-60,"elapsed":435,"user":{"displayName":"Jacqueline Ennis","photoUrl":"https://lh6.googleusercontent.com/-V_6i4Q83WGo/AAAAAAAAAAI/AAAAAAAADjY/vcjM0UgblwE/s64/photo.jpg","userId":"16264072060146161880"}},"colab":{"base_uri":"https://localhost:8080/","height":89}},"cell_type":"code","source":["# get all images from the new standard folder\n","dataset = os.listdir(os.path.join(DRIVE_FOLDER, MAIN_FOLDER))\n","print(\"There are\", len(dataset), \"total images in this dataset.\")\n","\n","t = 0.8\n","#split_train_dev_test(dataset, t)\n","\n","trainers = os.listdir(os.path.join(DRIVE_FOLDER, \"train\"))\n","print(\"There are\", len(trainers), \"training images.\")\n","\n","devs = os.listdir(os.path.join(DRIVE_FOLDER, \"dev\"))\n","print(\"There are\", len(devs), \"development images.\")\n","\n","testers = os.listdir(os.path.join(DRIVE_FOLDER, \"test\"))\n","print(\"There are\", len(testers), \"test images.\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["There are 5539 total images in this dataset.\n","There are 4432 training images.\n","There are 554 development images.\n","There are 553 test images.\n"],"name":"stdout"}]},{"metadata":{"id":"787H1Nn0mQ4M","colab_type":"code","colab":{}},"cell_type":"code","source":["def set_logger(log_path):\n","    \"\"\"Sets the logger to log info in terminal and file `log_path`.\n","    In general, it is useful to have a logger so that every output to the terminal is saved\n","    in a permanent file. Here we save it to `model_dir/train.log`.\n","    Example:\n","    ```\n","    logging.info(\"Starting training...\")\n","    ```\n","    Args:\n","        log_path: (string) where to log\n","    \"\"\"\n","    logger = logging.getLogger()\n","    logger.setLevel(logging.INFO)\n","\n","    if not logger.handlers:\n","        # Logging to a file\n","        file_handler = logging.FileHandler(log_path)\n","        file_handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s: %(message)s'))\n","        logger.addHandler(file_handler)\n","\n","        # Logging to console\n","        stream_handler = logging.StreamHandler()\n","        stream_handler.setFormatter(logging.Formatter('%(message)s'))\n","        logger.addHandler(stream_handler)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JDWFgRSCpc5c","colab_type":"code","colab":{}},"cell_type":"code","source":["def _parse_function(filename, label, size):\n","    \"\"\"Obtain the image from the filename (for both training and validation).\n","    The following operations are applied:\n","        - Decode the image from png format\n","        - Convert to float and to range [0, 1]\n","    \"\"\"\n","    image_string = tf.read_file(filename)\n","\n","    image_decoded = tf.image.decode_png(image_string, channels=3)\n","\n","    # This will convert to float values in [0, 1]\n","    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n","\n","    resized_image = tf.image.resize_images(image, [size, size])\n","\n","    return resized_image, label\n","  \n","def train_preprocess(image, label, use_random_flip):\n","    \"\"\"Image preprocessing for training.\n","    Apply the following operations:\n","        - Horizontally flip the image with probability 1/2\n","        - Apply random brightness and saturation\n","    \"\"\"\n","    if use_random_flip:\n","        image = tf.image.random_flip_left_right(image)\n","\n","    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n","    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n","\n","    # Make sure the image is still in [0, 1]\n","    image = tf.clip_by_value(image, 0.0, 1.0)\n","\n","    return image, label\n","  \n","def input_fn(is_training, filenames, labels, params):\n","    \"\"\"\n","    The filenames have format \"{speciesLabel}_{idNumber}.png\".\n","    For instance: \"Scentless Mayweed_29.png\".\n","    Args:\n","        is_training: (bool) whether to use the train or test pipeline.\n","                     At training, we shuffle the data and have multiple epochs\n","        filenames: (list) filenames of the images, as [\"data_dir/{label}_IMG_{id}.jpg\"...]\n","        labels: (list) corresponding list of species labels\n","        params: (Params) contains hyperparameters of the model (ex: `params.num_epochs`)\n","    \"\"\"\n","    \n","    num_samples = len(filenames)\n","    assert len(filenames) == len(labels), \"Filenames and labels should have same length\"\n","\n","    # Create a Dataset serving batches of images and labels\n","    # We don't repeat for multiple epochs because we always train and evaluate for one epoch\n","    parse_fn = lambda f, l: _parse_function(f, l, params[\"image_size\"])\n","    train_fn = lambda f, l: train_preprocess(f, l, params[\"use_random_flip\"])\n","\n","    if is_training:\n","        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(filenames), tf.constant(labels)))\n","            .shuffle(num_samples)  # whole dataset into the buffer ensures good shuffling\n","            .map(parse_fn, num_parallel_calls=params[\"num_parallel_calls\"])\n","            .map(train_fn, num_parallel_calls=params[\"num_parallel_calls\"])\n","            .batch(params[\"batch_size\"])\n","            .prefetch(1)  # make sure you always have one batch ready to serve\n","        )\n","    else:\n","        dataset = (tf.data.Dataset.from_tensor_slices((tf.constant(filenames), tf.constant(labels)))\n","            .map(parse_fn)\n","            .batch(params[\"batch_size\"])\n","            .prefetch(1)  # make sure you always have one batch ready to serve\n","        )\n","\n","    # Create reinitializable iterator from dataset\n","    iterator = dataset.make_initializable_iterator()\n","    images, labels = iterator.get_next()\n","    iterator_init_op = iterator.initializer\n","\n","    inputs = {'images': images, 'labels': labels, 'iterator_init_op': iterator_init_op}\n","    return inputs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PUilYKFht3ha","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_model(is_training, inputs, params):\n","    \"\"\"Compute logits of the model (output distribution)\n","    Args:\n","        is_training: (bool) whether we are training or not\n","        inputs: (dict) contains the inputs of the graph (features, labels...)\n","                this can be `tf.placeholder` or outputs of `tf.data`\n","        params: (Params) hyperparameters\n","    Returns:\n","        output: (tf.Tensor) output of the model\n","    \"\"\"\n","    images = inputs['images']\n","\n","    assert images.get_shape().as_list() == [None, params[\"image_size\"], params[\"image_size\"], 3]\n","\n","    out = images\n","    # Define the number of channels of each convolution\n","    # For each block, we do: 3x3 conv -> batch norm -> relu -> 2x2 maxpool\n","    num_channels = params[\"num_channels\"]\n","    bn_momentum = params[\"bn_momentum\"]\n","    channels = [num_channels, num_channels * 2, num_channels * 4, num_channels * 8]\n","    for i, c in enumerate(channels):\n","        with tf.variable_scope('block_{}'.format(i+1)):\n","            out = tf.layers.conv2d(out, c, 3, padding='same')\n","            if params[\"use_batch_norm\"]:\n","                out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n","            out = tf.nn.relu(out)\n","            out = tf.layers.max_pooling2d(out, 2, 2)\n","\n","    assert out.get_shape().as_list() == [None, 4, 4, num_channels * 8]\n","\n","    out = tf.reshape(out, [-1, 4 * 4 * num_channels * 8])\n","    with tf.variable_scope('fc_1'):\n","        out = tf.layers.dense(out, num_channels * 8)\n","        if params[\"use_batch_norm\"]:\n","            out = tf.layers.batch_normalization(out, momentum=bn_momentum, training=is_training)\n","        out = tf.nn.relu(out)\n","    with tf.variable_scope('fc_2'):\n","        logits = tf.layers.dense(out, params[\"num_labels\"])\n","        \n","    return logits\n","\n","\n","def model_fn(mode, inputs, params, reuse=False):\n","    \"\"\"Model function defining the graph operations.\n","    Args:\n","        mode: (string) can be 'train' or 'eval'\n","        inputs: (dict) contains the inputs of the graph (features, labels...)\n","                this can be `tf.placeholder` or outputs of `tf.data`\n","        params: (Params) contains hyperparameters of the model (ex: `params.learning_rate`)\n","        reuse: (bool) whether to reuse the weights\n","    Returns:\n","        model_spec: (dict) contains the graph operations or nodes needed for training / evaluation\n","    \"\"\"\n","    is_training = (mode == 'train')\n","    labels = inputs['labels']\n","    labels = tf.cast(labels, tf.int64)\n","\n","    # -----------------------------------------------------------\n","    # MODEL: define the layers of the model\n","    with tf.variable_scope('model', reuse=reuse):\n","        # Compute the output distribution of the model and the predictions\n","        logits = build_model(is_training, inputs, params)\n","        predictions = tf.argmax(logits, 1)\n","\n","    # Define loss and accuracy\n","    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n","    accuracy = tf.reduce_mean(tf.cast(tf.equal(labels, predictions), tf.float32))\n","\n","    # Define training step that minimizes the loss with the Adam optimizer\n","    if is_training:\n","        optimizer = tf.train.AdamOptimizer(params[\"learning_rate\"])\n","        global_step = tf.train.get_or_create_global_step()\n","        if params[\"use_batch_norm\"]:\n","            # Add a dependency to update the moving mean and variance for batch normalization\n","            with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n","                train_op = optimizer.minimize(loss, global_step=global_step)\n","        else:\n","            train_op = optimizer.minimize(loss, global_step=global_step)\n","\n","\n","    # -----------------------------------------------------------\n","    # METRICS AND SUMMARIES\n","    # Metrics for evaluation using tf.metrics (average over whole dataset)\n","    with tf.variable_scope(\"metrics\"):\n","        metrics = {\n","            'accuracy': tf.metrics.accuracy(labels=labels, predictions=tf.argmax(logits, 1)),\n","            'loss': tf.metrics.mean(loss)\n","        }\n","\n","    # Group the update ops for the tf.metrics\n","    update_metrics_op = tf.group(*[op for _, op in metrics.values()])\n","\n","    # Get the op to reset the local variables used in tf.metrics\n","    metric_variables = tf.get_collection(tf.GraphKeys.LOCAL_VARIABLES, scope=\"metrics\")\n","    metrics_init_op = tf.variables_initializer(metric_variables)\n","\n","    # Summaries for training\n","    tf.summary.scalar('loss', loss)\n","    tf.summary.scalar('accuracy', accuracy)\n","    tf.summary.image('train_image', inputs['images'])\n","\n","    #TODO: if mode == 'eval': ?\n","    # Add incorrectly labeled images\n","    mask = tf.not_equal(labels, predictions)\n","\n","    # Add a different summary to know how they were misclassified\n","    for label in range(0, params[\"num_labels\"]):\n","        mask_label = tf.logical_and(mask, tf.equal(predictions, label))\n","        incorrect_image_label = tf.boolean_mask(inputs['images'], mask_label)\n","        tf.summary.image('incorrectly_labeled_{}'.format(label), incorrect_image_label)\n","\n","    # -----------------------------------------------------------\n","    # MODEL SPECIFICATION\n","    # Create the model specification and return it\n","    # It contains nodes or operations in the graph that will be used for training and evaluation\n","    model_spec = inputs\n","    model_spec['variable_init_op'] = tf.global_variables_initializer()\n","    model_spec[\"predictions\"] = predictions\n","    model_spec['loss'] = loss\n","    model_spec['accuracy'] = accuracy\n","    model_spec['metrics_init_op'] = metrics_init_op\n","    model_spec['metrics'] = metrics\n","    model_spec['update_metrics'] = update_metrics_op\n","    model_spec['summary_op'] = tf.summary.merge_all()\n","\n","    if is_training:\n","        model_spec['train_op'] = train_op\n","\n","    return model_spec"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pVHJjPrkBn4D","colab_type":"code","colab":{}},"cell_type":"code","source":["def train_sess(sess, model_spec, num_steps, writer, params):\n","    \"\"\"Train the model on `num_steps` batches\n","    Args:\n","        sess: (tf.Session) current session\n","        model_spec: (dict) contains the graph operations or nodes needed for training\n","        num_steps: (int) train for this number of batches\n","        writer: (tf.summary.FileWriter) writer for summaries\n","        params: (Params) hyperparameters\n","    \"\"\"\n","    # Get relevant graph operations or nodes needed for training\n","    loss = model_spec['loss']\n","    train_op = model_spec['train_op']\n","    update_metrics = model_spec['update_metrics']\n","    metrics = model_spec['metrics']\n","    summary_op = model_spec['summary_op']\n","    global_step = tf.train.get_global_step()\n","\n","    # Load the training dataset into the pipeline and initialize the metrics local variables\n","    sess.run(model_spec['iterator_init_op'])\n","    sess.run(model_spec['metrics_init_op'])\n","\n","    # Use tqdm for progress bar\n","    t = trange(num_steps)\n","    for i in t:\n","        # Evaluate summaries for tensorboard only once in a while\n","        if i % params[\"save_summary_steps\"] == 0:\n","            # Perform a mini-batch update\n","            _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,\n","                                                              summary_op, global_step])\n","            # Write summaries for tensorboard\n","            writer.add_summary(summ, global_step_val)\n","        else:\n","            _, _, loss_val = sess.run([train_op, update_metrics, loss])\n","        # Log the loss in the tqdm progress bar\n","        t.set_postfix(loss='{:05.3f}'.format(loss_val))\n","\n","\n","    metrics_values = {k: v[0] for k, v in metrics.items()}\n","    metrics_val = sess.run(metrics_values)\n","    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_val.items())\n","    logging.info(\"- Train metrics: \" + metrics_string)\n","    \n","def train_and_evaluate(train_model_spec, eval_model_spec, model_dir, params, restore_from=None):\n","    \"\"\"Train the model and evaluate every epoch.\n","    Args:\n","        train_model_spec: (dict) contains the graph operations or nodes needed for training\n","        eval_model_spec: (dict) contains the graph operations or nodes needed for evaluation\n","        model_dir: (string) directory containing config, weights and log\n","        params: (Params) contains hyperparameters of the model.\n","                Must define: num_epochs, train_size, batch_size, eval_size, save_summary_steps\n","        restore_from: (string) directory or file containing weights to restore the graph\n","    \"\"\"\n","    # Initialize tf.Saver instances to save weights during training\n","    last_saver = tf.train.Saver() # will keep last 5 epochs\n","    best_saver = tf.train.Saver(max_to_keep=1)  # only keep 1 best checkpoint (best on eval)\n","    begin_at_epoch = 0\n","\n","    with tf.Session() as sess:\n","        # Initialize model variables\n","        sess.run(train_model_spec['variable_init_op'])\n","\n","        # Reload weights from directory if specified\n","        if restore_from is not None:\n","            logging.info(\"Restoring parameters from {}\".format(restore_from))\n","            if os.path.isdir(restore_from):\n","                restore_from = tf.train.latest_checkpoint(restore_from)\n","                begin_at_epoch = int(restore_from.split('-')[-1])\n","            last_saver.restore(sess, restore_from)\n","\n","        # For tensorboard (takes care of writing summaries to files)\n","        train_writer = tf.summary.FileWriter(os.path.join(model_dir, 'train_summaries'), sess.graph)\n","        eval_writer = tf.summary.FileWriter(os.path.join(model_dir, 'eval_summaries'), sess.graph)\n","\n","        best_eval_acc = 0.0\n","        for epoch in range(begin_at_epoch, begin_at_epoch + params[\"num_epochs\"]):\n","            # Run one epoch\n","            logging.info(\"Epoch {}/{}\".format(epoch + 1, begin_at_epoch + params[\"num_epochs\"]))\n","            # Compute number of batches in one epoch (one full pass over the training set)\n","            num_steps = (params[\"train_size\"] + params[\"train_size\"] - 1) // params[\"batch_size\"]\n","            train_sess(sess, train_model_spec, num_steps, train_writer, params)\n","\n","            # Save weights\n","            last_save_path = os.path.join(model_dir, 'last_weights', 'after-epoch')\n","            last_saver.save(sess, last_save_path, global_step=epoch + 1)\n","\n","            # Evaluate for one epoch on validation set\n","            num_steps = (params[\"eval_size\"] + params[\"batch_size\"] - 1) // params[\"batch_size\"]\n","            metrics = evaluate_sess(sess, eval_model_spec, num_steps, eval_writer)\n","\n","            # If best_eval, best_save_path\n","            eval_acc = metrics['accuracy']\n","            if eval_acc >= best_eval_acc:\n","                # Store new best accuracy\n","                best_eval_acc = eval_acc\n","                # Save weights\n","                best_save_path = os.path.join(model_dir, 'best_weights', 'after-epoch')\n","                best_save_path = best_saver.save(sess, best_save_path, global_step=epoch + 1)\n","                logging.info(\"- Found new best accuracy, saving in {}\".format(best_save_path))\n","                # Save best eval metrics in a json file in the model directory\n","                best_json_path = os.path.join(model_dir, \"metrics_eval_best_weights.json\")\n","                save_dict_to_json(metrics, best_json_path)\n","\n","            # Save latest eval metrics in a json file in the model directory\n","            last_json_path = os.path.join(model_dir, \"metrics_eval_last_weights.json\")\n","            save_dict_to_json(metrics, last_json_path)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wnu-wNb6AvRG","colab_type":"code","outputId":"6023758b-ec0f-4e55-eae6-c1dde561d7fc","executionInfo":{"status":"ok","timestamp":1550644075232,"user_tz":-60,"elapsed":553,"user":{"displayName":"Jacqueline Ennis","photoUrl":"https://lh6.googleusercontent.com/-V_6i4Q83WGo/AAAAAAAAAAI/AAAAAAAADjY/vcjM0UgblwE/s64/photo.jpg","userId":"16264072060146161880"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"cell_type":"code","source":["tf.set_random_seed(101)\n","\n","params = {\n","    \"learning_rate\": 0.001,\n","    \"batch_size\": 32,\n","    \"num_epochs\": 10,\n","\n","    \"num_channels\": 16,\n","    \"use_batch_norm\": True,\n","    \"bn_momentum\": 0.9,\n","\n","    \"image_size\": 64,\n","    \"use_random_flip\": True,\n","    \"num_labels\": 12,\n","\n","    \"num_parallel_calls\": 4,\n","    \"save_summary_steps\": 1\n","}\n","\n","set_logger(os.path.join(DRIVE_FOLDER, 'train.log'))\n","\n","# Create the input data pipeline\n","logging.info(\"Creating the datasets...\")\n","\n","train_data_dir = os.path.join(DRIVE_FOLDER, \"train\")\n","# test_data_dir = os.path.join(DRIVE_FOLDER, \"test\")\n","eval_data_dir = os.path.join(DRIVE_FOLDER, \"dev\")\n","\n","# Get the filenames from the train and dev sets\n","train_filenames = [os.path.join(train_data_dir, img_id) for img_id in os.listdir(train_data_dir) if img_id.endswith('.png')]\n","# test_filenames = [os.path.join(test_data_dir, img_id) for img_id in os.listdir(test_data_dir) if img_id.endswith('.png')]\n","eval_filenames = [os.path.join(eval_data_dir, img_id) for img_id in os.listdir(eval_data_dir) if img_id.endswith('.png')]\n","\n","train_labels = [(img_id.split('_')[0]) for img_id in os.listdir(train_data_dir)]\n","# test_labels = [(img_id.split('_')[0]) for img_id in os.listdir(test_data_dir)]\n","eval_labels = [(img_id.split('_')[0]) for img_id in os.listdir(eval_data_dir)]\n","params['train_size'] = len(train_filenames)\n","# params['test_size'] = len(test_filenames)\n","params['eval_size'] = len(eval_filenames)\n","\n","# encode string labels to numerical\n","le = preprocessing.LabelEncoder()\n","\n","train_labels = le.fit_transform(train_labels)\n","eval_labels = le.transform(eval_labels)\n","print(\"Labels after encoding:\", train_labels)\n","\n","train_size = len(train_filenames)\n","eval_size = len(eval_filenames)\n","\n","print(\"There are\", train_size, \"training images and\", eval_size, \"development images.\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Creating the datasets...\n"],"name":"stderr"},{"output_type":"stream","text":["Labels after encoding: [ 3  7  2 ...  9  3 10]\n","There are 4432 training images and 554 development images.\n"],"name":"stdout"}]},{"metadata":{"id":"vLg443cYH73r","colab_type":"code","outputId":"51156c22-1c99-47ca-f1ce-e1657e3e84ad","executionInfo":{"status":"ok","timestamp":1550644077598,"user_tz":-60,"elapsed":2508,"user":{"displayName":"Jacqueline Ennis","photoUrl":"https://lh6.googleusercontent.com/-V_6i4Q83WGo/AAAAAAAAAAI/AAAAAAAADjY/vcjM0UgblwE/s64/photo.jpg","userId":"16264072060146161880"}},"colab":{"base_uri":"https://localhost:8080/","height":919}},"cell_type":"code","source":["# Create the two iterators over the two datasets\n","train_inputs = input_fn(True, train_filenames, train_labels, params)\n","eval_inputs = input_fn(True, eval_filenames, eval_labels, params)\n","\n","\n","# Define the model\n","logging.info(\"Creating the model...\")\n","train_model_spec = model_fn('train', train_inputs, params)\n","eval_model_spec = model_fn('eval', eval_inputs, params, reuse=True)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-3-e9bd6209bbc2>:71: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n"],"name":"stdout"},{"output_type":"stream","text":["From <ipython-input-3-e9bd6209bbc2>:71: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.\n","Creating the model...\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-7c9b2304dd88>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.conv2d instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From <ipython-input-4-7c9b2304dd88>:23: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.conv2d instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-7c9b2304dd88>:25: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.batch_normalization instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From <ipython-input-4-7c9b2304dd88>:25: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.batch_normalization instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-7c9b2304dd88>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.max_pooling2d instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From <ipython-input-4-7c9b2304dd88>:27: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.max_pooling2d instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-4-7c9b2304dd88>:33: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From <ipython-input-4-7c9b2304dd88>:33: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.dense instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stderr"}]},{"metadata":{"id":"Xj4Z-226HYMA","colab_type":"code","outputId":"fe392b56-47d9-4071-fe2b-259d98072adc","executionInfo":{"status":"error","timestamp":1550644099936,"user_tz":-60,"elapsed":24436,"user":{"displayName":"Jacqueline Ennis","photoUrl":"https://lh6.googleusercontent.com/-V_6i4Q83WGo/AAAAAAAAAAI/AAAAAAAADjY/vcjM0UgblwE/s64/photo.jpg","userId":"16264072060146161880"}},"colab":{"base_uri":"https://localhost:8080/","height":1452}},"cell_type":"code","source":["# Train the model\n","logging.info(\"Starting training for {} epoch(s)\".format(params[\"num_epochs\"]))\n","train_and_evaluate(train_model_spec, eval_model_spec, DRIVE_FOLDER, params, None)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Starting training for 10 epoch(s)\n","Epoch 1/10\n"," 16%|█▋        | 45/276 [00:20<01:42,  2.25it/s, loss=1.956]"],"name":"stderr"},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[{{node IteratorGetNext}}]]","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-cfdde9377f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training for {} epoch(s)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_model_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDRIVE_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-1d3a558b94cd>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(train_model_spec, eval_model_spec, model_dir, params, restore_from)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Compute number of batches in one epoch (one full pass over the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_size\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mtrain_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Save weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-1d3a558b94cd>\u001b[0m in \u001b[0;36mtrain_sess\u001b[0;34m(sess, model_spec, num_steps, writer, params)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Perform a mini-batch update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             _, _, loss_val, summ, global_step_val = sess.run([train_op, update_metrics, loss,\n\u001b[0;32m---> 29\u001b[0;31m                                                               summary_op, global_step])\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Write summaries for tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msumm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Expected image (JPEG, PNG, or GIF), got empty file\n\t [[{{node DecodePng}}]]\n\t [[node IteratorGetNext (defined at <ipython-input-3-e9bd6209bbc2>:72) ]]"]}]},{"metadata":{"id":"kCXI5s9RmO5O","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}